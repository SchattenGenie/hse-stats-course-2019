{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка гипотез. \n",
    "\n",
    "# 1. Задачка на монетку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построить критерий для проверки гипотезы $H_0: p \\!= 1/2$ при альтернативной гипотезе $H_1 : p \\neq  1/2$ по результатам восьми испытаний, подчиняющихся схеме Бернулли. Вероятность ошибки первого рода $\\alpha $ положить равной 0.05.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ошибки\n",
    "\n",
    "![](./stat_errors.png)\n",
    "\n",
    "В статистике различаются два вида ошибок: \n",
    "  * ошибка первого рода -- когда мы отвергаем гипотезу, а она верна;\n",
    "  * ошибка второго рода -- когда мы __не__ отвергаем гипотезу, а она не верна.\n",
    "    (т.е. неверно приняли приняли нулевую гипотезу)\n",
    "  \n",
    "  \n",
    "![](https://openlab-flowers.inria.fr/uploads/default/original/2X/7/703b9d4e3037b266e8fc6b20e020eb84d4405a80.png)\n",
    "\n",
    "\n",
    "$$p\\mathrm{-value} = 1 - \\int\\limits_{-\\infty}^{t_{\\alpha}} p_{H_0}(t) dt$$\n",
    "\n",
    "\n",
    "Теперь мы фиксируем ошибку первого рода:\n",
    "\n",
    "$$\\alpha=0.05$$\n",
    "\n",
    "Что означает следующее:\n",
    "\n",
    "$$P(x < x' | H_0) = P(x > x'' | H_0) \\leq \\frac{\\alpha}{2} = 0.025$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, expon, beta\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy, scipy.stats\n",
    "from scipy.stats import binom\n",
    "import scipy, scipy.stats\n",
    "x = np.arange(0, 9)\n",
    "pmf = binom.pmf(x, 8, 0.5)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.bar(x, pmf, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 9)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "pmf = binom.pmf(x, 8, 0.5)\n",
    "plt.bar(x, pmf, alpha=0.5, label='p=0.5')\n",
    "\n",
    "pmf = binom.pmf(x, 8, 0.2)\n",
    "plt.bar(x, pmf, alpha=0.5, label='p=0.2')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = binom(n=8, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr.pmf(0), distr.pmf(0) < alpha / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr.pmf(1), distr.pmf(1) < alpha / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-1, 10)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "pmf = binom.cdf(x, 8, 0.5)\n",
    "plt.bar(x, pmf, alpha=0.5, label='p=0.5')\n",
    "\n",
    "plt.plot(x, alpha * np.ones(len(x)) / 2, 'k', linewidth=3, linestyle='--')\n",
    "\n",
    "plt.xlim(-0.5, 8.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Получается что не получается построить гипотезу!\n",
    "\n",
    "<Что будем делать?>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hypothesis(samples):\n",
    "    rejected = <YOUR CODE>\n",
    "    rejected += <YOUR CODE>\n",
    "    rejected += (samples == 0).sum()\n",
    "    rejected += (samples == 8).sum()\n",
    "    return rejected / len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = binom(n=8, p=0.5)\n",
    "samples = distr.rvs(size=10000000)\n",
    "print('Эмпирическая ошибка первого рода: {}'.format(test_hypothesis(samples=samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.linspace(0, 1, 100)\n",
    "betas = []\n",
    "for p in ps:\n",
    "    distr = binom(n=8, p=p)\n",
    "    samples = distr.rvs(size=100000)\n",
    "    betas.append(1 - test_hypothesis(samples=samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.plot(ps, betas)\n",
    "plt.ylabel('Вероятность ошибки II рода')\n",
    "plt.xlabel('Истинная p распределения')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Неожиданный момент!\n",
    "\n",
    "Теперь, зная про критерий Неймана-Пирсона, мы может с уверенностью сказать что наш критерий является наиболее мощным среди всех критерией размера $\\alpha$!\n",
    "\n",
    "Действительно, выпишем статистику Неймана-Пирсона:\n",
    "\n",
    "$$ T = \\frac{C_n^k p^k (1 - p)^k}{C_n^k p_0^n} = \\frac{p^k (1 - p)^k}{p_0^n}$$\n",
    "\n",
    "Тогда нужно всего лишь найти threshold для данной статистики:\n",
    " \n",
    "$$\\mathbb{P}_{p_0} \\left( \\frac{p^k (1 - p)^k}{p_0^n} \\geq \\mathrm{threshold} \\right) = \\alpha$$\n",
    "\n",
    "А это как раз соответствует нахождению плотности критических $k$ в хвостиках распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Генерация тестовых выборок с точностями (prediction accuracy) для предсказания рака груди\n",
    "\n",
    "https://arxiv.org/abs/1806.08295\n",
    "\n",
    "The features from the data set describe characteristics of the cell nuclei and are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. As described in [UCI Machine Learning Repository][1], the attribute informations are:\n",
    "\n",
    "1. ID number\n",
    "2. Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "3 - 32  Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "* a) radius (mean of distances from center to points on the perimeter)\n",
    "* b) texture (standard deviation of gray-scale values)\n",
    "* c) perimeter\n",
    "* d) area\n",
    "* e) smoothness (local variation in radius lengths)\n",
    "* f) compactness (perimeter^2 / area - 1.0)\n",
    "* g) concavity (severity of concave portions of the contour)\n",
    "* h) concave points (number of concave portions of the contour)\n",
    "* i) symmetry\n",
    "* j) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "\n",
    "  [1]: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv');\n",
    "\n",
    "print(\"\\n \\t The data frame has {0[0]} rows and {0[1]} columns. \\n\".format(df.shape))\n",
    "df.drop(df.columns[[-1, 0]], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  count how many diagnosis are malignant (M) and how many are benign (B)\n",
    "diagnosis_all = list(df.shape)[0]\n",
    "diagnosis_categories = list(df['diagnosis'].value_counts())\n",
    "\n",
    "print(\"\\n \\t The data has {} diagnosis, {} malignant and {} benign.\".format(diagnosis_all, \n",
    "                                                                            diagnosis_categories[0], \n",
    "                                                                            diagnosis_categories[1]))\n",
    "features_mean = list(df.columns[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "color_dic = {'M':'red', 'B':'blue'}\n",
    "colors = df['diagnosis'].map(lambda x: color_dic.get(x))\n",
    "\n",
    "sm = pd.plotting.scatter_matrix(df[features_mean], c=colors, alpha=0.4, figsize=((15,15)));\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "bins = 12\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, feature in enumerate(features_mean):\n",
    "    rows = int(len(features_mean)/2)\n",
    "    \n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    \n",
    "    sns.distplot(df[df['diagnosis']=='M'][feature], bins=bins, color='red', label='M');\n",
    "    sns.distplot(df[df['diagnosis']=='B'][feature], bins=bins, color='blue', label='B');\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the models\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "# binarasing the diagnosis\n",
    "diag_map = {'M':1, 'B':0}\n",
    "df['diagnosis'] = df['diagnosis'].map(diag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,features_mean]\n",
    "y = df.loc[:, 'diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "accuracy_all = []\n",
    "cvs_all = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тренируем два классификатора - Байес и KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "scores_knn = cross_val_score(clf, X, y, cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=100))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "accuracy_all.append(accuracy_score(prediction, y_test))\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores_knn), np.std(scores_knn) * 2))\n",
    "print(\"Execution time: {0:.5} seconds \\n\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Naive Bayes algorithm applies Bayes’ theorem with the assumption of independence between every pair of features.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "scores_gnb = cross_val_score(clf, X, y, cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=100))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "accuracy_all.append(accuracy_score(prediction, y_test))\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores_gnb), np.std(scores_gnb)*2))\n",
    "print(\"Execution time: {0:.5} seconds \\n\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Оценка ошибки второго рода оценки $H_0$\n",
    "\n",
    "![](./stat_errors.png)\n",
    "\n",
    "В статистике различаются два вида ошибок: \n",
    "  * ошибка первого рода -- когда мы отвергаем гипотезу, а она верна;\n",
    "  * ошибка второго рода -- когда мы __не__ отвергаем гипотезу, а она не верна.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Тест Уелча](http://www.statistics4u.com/fundstat_eng/img/hl_explain_welch_test.png)\n",
    "  \n",
    "  \n",
    "$$t = \\frac{x_{diff}}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}}}$$\n",
    "\n",
    "$$v \\approx \\frac{\\left(\\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}\\right)^2}{\\frac{s_1^4}{N_1^2 (N_1 - 1)} + \\frac{s_2^4}{N_2^2 (N_2 - 1)}}$$\n",
    "\n",
    "\n",
    "#### Реализация теста Уелча\n",
    "https://github.com/flowersteam/rl-difference-testing/blob/master/test_RL_difference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * `welch_test` - реализация теста Уельча с помощью `stats.ttest_ind`;\n",
    "  * `compute_beta` - функция для подсчёта вероятности ошибки II рода.\n",
    "  * `empirical_false_pos_rate` - функция для ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_test(data1, data2, alpha=0.05, tail=2):\n",
    "    \"\"\"\n",
    "    Wraps around ttest_ind function of scipy, without assuming equal variances.\n",
    "    Params\n",
    "    ------\n",
    "    - data1 (ndarray of dim 1)\n",
    "    The performance measures of Algo1.\n",
    "    - data2 (ndarray of dim 1)\n",
    "    The performance measures of Algo2.\n",
    "    - alpha (float in ]0,1[)\n",
    "    The significance level used by the Welch's t-test.\n",
    "    - tail (1 or 2)\n",
    "    Perform a one tail or two tail test.\n",
    "    \"\"\"\n",
    "    assert tail == 1 or tail == 2, \"tail should be one or two, referring to the one-sided or two-sided t-test.\"\n",
    "    data1 = data1.squeeze()\n",
    "    data2 = data2.squeeze()\n",
    "    assert alpha < 1 and alpha > 0, \"alpha should be between 0 and 1\"\n",
    "\n",
    "    t, p = stats.ttest_ind(data1, data2, equal_var=False)\n",
    "\n",
    "    if tail == 1:\n",
    "        alpha = 2 * alpha\n",
    "    if p <= alpha:\n",
    "        if t < 0:\n",
    "            print(\"\\n\\n Result of the Welch's t-test at level %05g: μ2>μ1, the test passed with p-value = %05g.\" % (alpha, p))\n",
    "        else:\n",
    "            print(\"\\n\\n Result of the Welch's t-test level %05g: μ1>μ2, the test passed with p-value = %05g.\" % (alpha, p))\n",
    "    else:\n",
    "        print(\"\\n\\n Results of the Welch's t-test level %05g: there is not enough evidence to prove any order relation between μ1 and μ2.\" % alpha)\n",
    "    print(\"Welch's t-test done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welch_test(scores_gnb, scores_knn, alpha, tail=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def empirical_false_pos_rate(data, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute and plot empirical estimates of the probability of type-I error given a list of performance measures.\n",
    "    If this list is of size N_data\n",
    "    This is done for N=2:floor(N_data/2). Two different tests are used: the usual t-test and the\n",
    "    Welch's t-test, both with significance level alpha.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    - data (ndarray of dim 1)\n",
    "    The performance measures of the considered algorithm.\n",
    "    - alpha (float in ]0,1[)\n",
    "    The significance level used by the two tests.\n",
    "    \"\"\"\n",
    "    sizes = np.arange(2, data.size // 2, 2)\n",
    "    nb_reps = 1000\n",
    "    results = np.zeros([nb_reps, len(sizes), 2])\n",
    "    blue = [0, 0.447, 0.7410, 1]\n",
    "    orange = [0.85, 0.325, 0.098, 1]\n",
    "\n",
    "    for i_n, n in tqdm(enumerate(sizes)):\n",
    "        ind = list(range(2 * n))\n",
    "        for rep in range(nb_reps):\n",
    "            # take two groups of size n in data, at random\n",
    "            np.random.shuffle(ind)\n",
    "            sample_1 = data[ind[:n]]\n",
    "            sample_2 = data[ind[n:2 * n]]\n",
    "            # perform the two-tail Welch's t-test\n",
    "            results[rep, i_n, 0] = stats.ttest_ind(sample_1, sample_2, equal_var=False)[1] < alpha\n",
    "            # perform the two-tail t-test\n",
    "            results[rep, i_n, 1] = stats.ttest_ind(sample_1, sample_2, equal_var=True)[1] < alpha\n",
    "\n",
    "    res_mean = results.mean(axis=0)\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.plot(sizes, alpha * np.ones(len(sizes)), c='k', linewidth=5, linestyle='--')\n",
    "    plt.plot(sizes, res_mean[:, 0], color=blue, linewidth=4, label=u\"α=%02d Welch's $t$-test\".format(alpha))\n",
    "    plt.plot(sizes, res_mean[:, 1], color=orange, linewidth=4, label=u\"α=%02d Usual's $t$-test\".format(alpha))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('sample size (N)')\n",
    "    plt.ylabel('P(false positive)')\n",
    "    \n",
    "    plt.title(u'Estimation of type-I error rate as a function of $N$ when $α=0.05$')\n",
    "    plt.show()\n",
    "    print(\n",
    "    \"\\n   Given N={} and α={}, you can expect false positive rates: \\n \\\n",
    "    For the Welch's t-test: {} \\n     For the ordinal t-test: {}.\".format(\n",
    "        data.size // 2, alpha, res_mean[-1, 0], res_mean[-1, 1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выборка случайно бьётся на равные части \n",
    "# и для них случайно делается разбиение \n",
    "# и смотрится на ошибку первого рода\n",
    "empirical_false_pos_rate(scores_knn, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_knn.mean() - scores_gnb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ошибка первого рода\n",
    "alpha = 0.05\n",
    "\n",
    "# ошибка первого рода\n",
    "beta_requirement = 0.2\n",
    "\n",
    "\n",
    "if scores_knn.mean() < scores_gnb.mean():\n",
    "    m_smaller = scores_knn.mean()\n",
    "else:\n",
    "    m_smaller = scores_gnb.mean()\n",
    "\n",
    "# размер выборки\n",
    "sample_size = range(2, 200, 2)\n",
    "\n",
    "# effect size -- какую разницу мы хотим уметь детектить?\n",
    "epsilon = np.array([0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) * m_smaller\n",
    "epsilon = epsilon.tolist()\n",
    "print(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_beta(epsilon, sample_size, alpha=0.05, data1=None, data2=None, s1=None, s2=None, beta_requirement=0.2):\n",
    "    \"\"\"\n",
    "    Computes the probability of type-II error (or false positive rate) beta to detect and effect size epsilon\n",
    "    when testing for a difference between performances of Algo1 versus Algo2, using a Welch's t-test\n",
    "    with significance alpha and sample size N.\n",
    "    Params\n",
    "    ------\n",
    "    - epsilon (int, float or list of int or float)\n",
    "    The effect size one wants to be able to detect.\n",
    "    - sample_size (int or list of int)\n",
    "    The sample size (assumed equal for both algorithms).\n",
    "    - alpha (float in ]0,1[)\n",
    "    The significance level used by the Welch's t-test.\n",
    "    - data1 (ndarray of dim 1)\n",
    "    The performance measures of Algo1. Optional if s1 is provided.\n",
    "    - data2 (ndarray of dim 1)\n",
    "    The performance measures of Algo2. Optional if s2 is provided.\n",
    "    - s1 (float)\n",
    "    The standard deviation of Algo1, optional if data1 is provided.\n",
    "    - s2 (float)\n",
    "    The standard deviation of Algo2, optional if data2 is provided.\n",
    "    - beta_requirement (float in ]0,1[, optional)\n",
    "    Requirements on the value of beta.\n",
    "    \"\"\"\n",
    "    print('\\n\\nComputing the false negative rate as a function of sample size, for various effect sizes ..')\n",
    "    assert alpha < 1 and alpha > 0, \"alpha must be in ]0,1[\"\n",
    "    assert data1 is not None or s1 is not None, \"data1 or s1 should be provided\"\n",
    "    assert data2 is not None or s2 is not None, \"data1 or s2 should be provided\"\n",
    "\n",
    "    if type(epsilon) is int or type(epsilon) is float:\n",
    "        epsilon = [epsilon]\n",
    "        n_eps = 1\n",
    "    else:\n",
    "        n_eps = len(epsilon)\n",
    "\n",
    "    if type(sample_size) is int:\n",
    "        sample_size = [sample_size]\n",
    "        n_sample_size = 1\n",
    "    else:\n",
    "        n_sample_size = len(sample_size)\n",
    "\n",
    "    if data1 is not None:\n",
    "        s1 = data1.std(ddof=1)\n",
    "    else:\n",
    "        s1 = s1\n",
    "\n",
    "    if data2 is not None:\n",
    "        s2 = data2.std(ddof=1)\n",
    "    else:\n",
    "        s2 = s2\n",
    "\n",
    "    results = np.zeros([n_sample_size, n_eps])\n",
    "    t_dist = stats.distributions.t\n",
    "\n",
    "    selected_sample_size = []\n",
    "    for i_diff, eps in enumerate(epsilon):\n",
    "        sample_size_found = False  # True if a previous sample size satisfied beta requirements for the current epsilon\n",
    "        for i_n, n in enumerate(sample_size):\n",
    "            nu = (s1 ** 2 + s2 ** 2) ** 2 * (n - 1) / (s1 ** 4 + s2 ** 4)\n",
    "            t_eps = eps / np.sqrt((s1 ** 2 + s2 ** 2) / n)\n",
    "            t_crit = t_dist.ppf(1 - alpha, nu)\n",
    "            results[i_n, i_diff] = t_dist.cdf(t_crit - t_eps, nu)\n",
    "            if results[i_n, i_diff] < beta_requirement and not sample_size_found:\n",
    "                sample_size_found = True\n",
    "                selected_sample_size.append(str(n))\n",
    "        if not sample_size_found:\n",
    "            selected_sample_size.append('>' + str(n))\n",
    "\n",
    "    eps_str = str()\n",
    "    for i in range(n_eps):\n",
    "        eps_str += '    ε = {}  -->  N: {} \\n '.format(epsilon[i], selected_sample_size[i])\n",
    "\n",
    "    print('\\nSample sizes satisfying β={} are:\\n {}'.format(beta_requirement, eps_str))\n",
    "    print('Done.')\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_beta(beta, epsilon, sample_size, beta_requirement=0.2):\n",
    "    \"\"\"\n",
    "    Plot the probability of type-II error beta as a function of the sample size, for various effect sizes epsilon\n",
    "    Params\n",
    "    ------\n",
    "    - beta (ndarray of shape (size(N), size(epsilon))\n",
    "    Contains values of beta for various epsilon and N\n",
    "    - epsilon (int, float or list of int or float)\n",
    "    The effect size one wants to be able to detect.\n",
    "    - sample_size (int or list of int)\n",
    "    The sample size (assumed equal for both algorithms).\n",
    "    - beta_requirement (float in ]0,1[, optional)\n",
    "    Requirements on the value of beta.\n",
    "    \"\"\"\n",
    "    if type(epsilon) is int or type(epsilon) is float:\n",
    "        epsilon = [epsilon]\n",
    "        n_eps = 1\n",
    "    else:\n",
    "        n_eps = len(epsilon)\n",
    "\n",
    "    if type(sample_size) is int:\n",
    "        sample_size = [sample_size]\n",
    "        n_sample_size = 1\n",
    "    else:\n",
    "        n_sample_size = len(sample_size)\n",
    "\n",
    "    try:\n",
    "        assert n_sample_size > 1\n",
    "    except:\n",
    "        print(\"Beta cannot be plotted as a function of only one sample size.\")\n",
    "        return\n",
    "\n",
    "    legend = [u'$β_{requirement}$']\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.plot(sample_size, beta_requirement * np.ones(n_sample_size), 'k', linewidth=3, linestyle='--')\n",
    "    plt.plot(sample_size, beta, linewidth=2)\n",
    "    if n_eps > 1:\n",
    "        print(epsilon)\n",
    "        legend += [u'ε = {}'.format(epsilon[i]) for i in range(n_eps)]\n",
    "    else:\n",
    "        print(epsilon)\n",
    "        legend += u'ε = {}'.format(epsilon[0])\n",
    "    plt.legend(legend)\n",
    "    plt.xlabel('sample size (N)')\n",
    "    plt.ylabel('P(false negative)')\n",
    "    plt.title(u'Estimation of type-II error rate as a function of ε and $N$')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чисто аналитический анализ\n",
    "# выборки используются \n",
    "# только чтобы посчитать std\n",
    "# разрешаем гипотезу относительно | \\mu_1 - \\mu_2 |\n",
    "\n",
    "beta = compute_beta(epsilon, sample_size, alpha, \n",
    "                    scores_knn, scores_gnb, beta_requirement=beta_requirement)\n",
    "\n",
    "plot_beta(beta, epsilon, sample_size, beta_requirement=beta_requirement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Парадокс Линдлей\n",
    "\n",
    "### Часть первая\n",
    "\n",
    "У нас есть наблюдения $\\{ x \\}_{i=1}^{n}$, которые подчиняются нормальному распределению:\n",
    "\n",
    "$$x_i \\sim \\mathcal{N}(x | \\mu, \\sigma^2),$$\n",
    "\n",
    "где $\\mu$ нам неизвестно, но известно априрное распределение на этот параметр:\n",
    "\n",
    "$$p(\\mu) = \\frac{1}{2} \\delta(\\mu | \\mu_0) + \\frac{1}{2} \\mathcal{N}(\\mu | \\mu_0, \\sigma^2)$$\n",
    "\n",
    "Мы пронаблюдали много $x_i$ и посчитали следующую статистику:\n",
    "\n",
    "$$\\bar{x} = \\mu_0 + 1.96 \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "### Задача. Протестируйте гипотезу:\n",
    "\n",
    "$$H_0:\\mu = \\mu_0 ~~~ \\mathrm{vs} ~~~ H_1:\\mu\\neq\\mu_0$$\n",
    "\n",
    "\n",
    "1. Как статист\n",
    "2. Как баесианец"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.\n",
    "xs = np.linspace(-5, 5, 1000)\n",
    "plt.figure(figsize=(12, 7), dpi=100)\n",
    "plt.plot(xs, norm(0, sigma).pdf(xs))\n",
    "plt.title('Normal distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зададим все параметры задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = 0.\n",
    "sigma = 1.\n",
    "n = 500\n",
    "\n",
    "x_bar = mu_0 + 1.96 * sigma / np.sqrt(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Протестируем её как статисты\n",
    "\n",
    "Очевидно, что:\n",
    "\n",
    "$$\\bar{x} \\sim ??? $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_bar - mu_0) / ( sigma / np.sqrt(n) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "norm.ppf(1 - alpha / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать выводы и проговорить их"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Протестируем её как байесианцы\n",
    "\n",
    "<Что делаем?>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_probability(mu_0, sigma, n):\n",
    "    <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.logspace(0.5, 5, 100).astype(int)\n",
    "bayesian_probabilities = []\n",
    "for n in ns:\n",
    "    bayesian_probabilities.append(bayesian_probability(mu_0, sigma, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7), dpi=100)\n",
    "plt.plot(ns, bayesian_probabilities)\n",
    "plt.title('Bayesian probability of H_0')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть вторая\n",
    "\n",
    "Рассмотрим более сложное априорное распределение:\n",
    "\n",
    "$$p(\\mu) = p_1 \\delta(\\mu | \\mu_0) + p_2 \\mathcal{N}(\\mu | \\mu_0, \\tau^2), \\tau \\rightarrow + \\infty$$\n",
    "\n",
    "\n",
    "И, кроме того, у нас есть только одно измерение $x$, т.е. $n = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_probability(x, mu_0, sigma, tau, p_1=0.5, p_2=0.5):\n",
    "    <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = np.logspace(0.5, 5, 40).astype(int)\n",
    "xs = np.linspace(0., 5., 10)\n",
    "bayesian_probabilities = []\n",
    "\n",
    "for tau in taus:\n",
    "    bayesian_probabilities.append(\n",
    "        [bayesian_probability(x, mu_0, sigma, tau) for x in xs]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_probabilities = np.array(bayesian_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12), dpi=200)\n",
    "for i, x in enumerate(xs):\n",
    "    plt.plot(taus, bayesian_probabilities[:, i], label='x={}'.format(x))\n",
    "plt.xscale('log')\n",
    "plt.title(\"Dependency of H_0 probability as function of x\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('H_0 probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
