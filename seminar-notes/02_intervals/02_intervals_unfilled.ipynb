{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Дельта-метод для оценки дисперсии функций от случайных величин\n",
    "\n",
    "Пусть $X$ распределено нормально и $\\sqrt{n}(\\bar{X}_n - \\mu) \\rightarrow N(0, \\sigma^2)$, тогда:\n",
    "\n",
    "$$\\sqrt{n}(g(\\bar{X}_n) - g(\\mu)) \\rightarrow \\mathcal{N}\\left(0, \\sigma^2 g'(\\mu)^2\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efron (1982) исследовал результаты сдачи двух экзаменов LSAT(law school admission test) и GPA (grade point average). Для 15 юридических школ он получил средние значения.\n",
    "\n",
    "1. Посчитайте корреляцию LSAT и GPA\n",
    "2. Если *(Х,Y)* двумерное нормальное распределение с коэффициетом корелляции *p* и где *r* - коэффициент корреляции сэмпла, тогда дельта-метод может быть использован, для того, чтобы показать:\n",
    "\n",
    "    $$ \\sqrt{n}(r-p) \\rightarrow N\\left(0, (1-p^2 )^2\\right)$$\n",
    "\n",
    "Докажем это. \n",
    "\n",
    "\n",
    "`<YOUR DOKAZATELSVO>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsat = [\n",
    "    576, 580, 653, 635, 555, \n",
    "    575, 558, 661, 545, 578, \n",
    "    651, 572, 666, 605, 594\n",
    "]\n",
    "\n",
    "gpa = [\n",
    "    3.39, 3.07, 3.12, 3.30, 3.00, \n",
    "    2.74, 2.81, 3.43, 2.76, 3.03, \n",
    "    3.36, 2.88, 3.44, 3.13, 2.96\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Посчитайте корреляцию LSAT и GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(lsat, gpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, _ = pearsonr(lsat, gpa)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_corr = <YOUR CODE>\n",
    "print(var_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Применение профилирования правдоподобия для оценки доверительных интервалов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Оценка погрешности параметров распределения\n",
    "\n",
    "Вспомним прошлую задачку с оценкой приспособленности разных представителей одного генотипа:\n",
    "\n",
    "$$\\hat{\\alpha} = 0.23,~~~ \\hat{\\beta} = 5.35$$\n",
    "\n",
    "\n",
    "Просэмплируем обучающую выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "alpha_true = 0.23\n",
    "beta_true = 5.35\n",
    "X = gamma.rvs(scale=1 / beta_true, a=alpha_true, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сделаем фитирование по всей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_alpha, _, fit_beta = gamma.fit(X, floc=0)\n",
    "fit_beta = 1 / fit_beta\n",
    "\n",
    "log_likelihood_max = gamma.logpdf(X, loc=0, a=fit_alpha, scale=1 / fit_beta).sum()\n",
    "print(fit_alpha, 1 / fit_beta, log_likelihood_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделаем профилирование правдоподобия по $\\alpha$\n",
    "\n",
    "\n",
    "Теперь более внимательно посмотрим на идею профилирования правдоподобия\n",
    "\n",
    "0. Находим максимум правдоподобия: $ \\mathcal{L}_{max} = \\max\\limits_{\\alpha, \\beta} \\mathcal{L}(\\alpha, \\beta)$\n",
    "\n",
    "1. Берём некоторый интервал $\\alpha \\in [\\alpha_1, \\alpha_2]$. \n",
    "\n",
    "2. Для каждого $\\alpha$ в этом интервале ищем максимум правдоподобия по параметру $\\beta$: \n",
    "\n",
    "$$\\mathcal{L}_\\alpha = \\max\\limits_{\\beta} \\mathcal{L}(\\alpha, \\beta) $$\n",
    "\n",
    "3. Строим график $\\alpha, \\mathcal{L}_\\alpha$ и делаем отсечение $\\mathcal{L}_\\alpha -  \\mathcal{L}_{max} >  -\\mathrm{cut\\_level}_{0.95}$, где \n",
    "\n",
    "\n",
    "$$\\mathrm{cut\\_level}_{0.95} = \\mathrm{<WHATS~~HERE?>} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi\n",
    "q = 0.95\n",
    "cut_level = <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_alphas = []\n",
    "fit_betas = []\n",
    "log_likelihoods = []\n",
    "alphas = np.linspace(0.1, 0.5, 100)\n",
    "for alpha in alphas:\n",
    "    fit_alpha, _, fit_beta = <YOUR CODE>\n",
    "    fit_betas.append(1 / fit_beta)\n",
    "    fit_alphas.append(fit_alpha)\n",
    "    log_likelihoods.append(gamma.logpdf(X, loc=0, a=alpha, scale=fit_beta).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(alphas, \n",
    "         (np.array((log_likelihoods)) - log_likelihood_max), \n",
    "         label='Likelihood value')\n",
    "plt.twinx()\n",
    "plt.plot(alphas, \n",
    "         (np.array((log_likelihoods)) - log_likelihood_max > -cut_level), \n",
    "         c='r', label='{} confidence inerval for alpha'.format(q))\n",
    "plt.axvline(alpha_true, label=r'True $\\alpha$ value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Профилируем likelihood по $\\beta$\n",
    "\n",
    "Делаем всё тоже самое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_alphas = []\n",
    "fit_betas = []\n",
    "log_likelihoods = []\n",
    "betas = np.linspace(0.001, 10, 100)\n",
    "for beta in betas:\n",
    "    fit_alpha, _, fit_beta = <YOUR CODE>\n",
    "    fit_betas.append(1 / fit_beta)\n",
    "    fit_alphas.append(fit_alpha)\n",
    "    log_likelihoods.append(gamma.logpdf(X, loc=0, a=fit_alpha, scale=fit_beta).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(betas, \n",
    "         (np.array((log_likelihoods)) - log_likelihood_max), \n",
    "         label='Likelihood value')\n",
    "plt.twinx()\n",
    "plt.plot(betas, \n",
    "         (np.array((log_likelihoods)) - log_likelihood_max > -cut_level), \n",
    "         c='r', label='{} confidence inerval for beta'.format(q))\n",
    "plt.axvline(beta_true, label=r'True $\\beta$ value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 2D профилирование likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, betas = np.meshgrid(alphas, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihoods = []\n",
    "for alpha, beta in zip(alphas.ravel(), betas.ravel()):\n",
    "    log_likelihoods.append(gamma.logpdf(X, loc=0, a=alpha, scale=1 / beta).sum() - log_likelihood_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(log_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihoods = np.array(log_likelihoods).reshape(alphas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "contours = plt.contour(alphas, \n",
    "                       betas, \n",
    "                       log_likelihoods, \n",
    "                       100, \n",
    "                       colors='black')\n",
    "plt.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "plt.imshow(log_likelihoods, extent=[0.1, 0.5, 0.001, 10], \n",
    "           aspect='auto', origin='lower', alpha=1.)\n",
    "plt.colorbar();\n",
    "\n",
    "plt.imshow(100 * (log_likelihoods < -cut_level), \n",
    "           extent=[0.1, 0.5, 0.001, 10], \n",
    "           aspect='auto', origin='lower',\n",
    "           cmap='RdGy', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Применение информации Фишера для оценки доверительных интервалов\n",
    "\n",
    "Информацию Фишера можно использовать для расчёта ошибок в оценке параметров и корреляций ошибок(что с профилированием likelihood гораздо менее тривиально сделать). \n",
    "\n",
    "Это делается из [неравенства Крамера-Рао](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D1%80%D0%B0%D0%B2%D0%B5%D0%BD%D1%81%D1%82%D0%B2%D0%BE_%D0%9A%D1%80%D0%B0%D0%BC%D0%B5%D1%80%D0%B0_%E2%80%94_%D0%A0%D0%B0%D0%BE), которое утверждает, что снизу дисперсия параметра ограничена диагональным членом матрицы обратной к матрице Фишера.\n",
    "\n",
    "Посмотрим на оценку дисперсии параметров некоторого периодичного сигнала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Фитирование кривой\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "#### Условие\n",
    "\n",
    "Пусть у нас есть некоторый сигнал:\n",
    "\n",
    "$$a(t) = A \\sin(2 \\pi f t + \\phi)$$\n",
    "\n",
    "\n",
    "При передаче он зашумляется. Предположим, что нам известна форма и параметры шума: $\\mathcal{N}(0, \\sigma^2)$. Тогда в момент времени $t_i$ мы будем наблюдать нечто следующее:\n",
    "\n",
    "$$\\hat{a}_i \\sim A \\sin(2 \\pi f t_i + \\phi) + \\mathcal{N}(0, \\sigma^2) = \\mathcal{N}\\left(A \\sin(2 \\pi f t_i + \\phi), \\sigma^2 \\right)$$\n",
    "\n",
    "\n",
    "#### Задача\n",
    "\n",
    "По набору измерений: $\\{t_i\\}_{i=1}^{N}$ и $\\{\\hat{a}_i\\}_{i=1}^{N}$(на самом деле, измерения нам даже не понадобятся) мы хотим:\n",
    "  * оценить ошибки восстановления параметров;\n",
    "  * оценить \"важность\" каждого измерения для оценки каждого из параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import derivative\n",
    "from scipy.optimize import curve_fit\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве сигнала мы возьмём синус со следующими параметрами: амплитуда, частота и фаза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal(t, a, f, ph):\n",
    "    return a * np.sin(2 * np.pi * f * t + ph)\n",
    "\n",
    "parameters = {\n",
    "    'a': 2,\n",
    "    'f': 8,\n",
    "    'ph': 0\n",
    "}\n",
    "noise = 0.1\n",
    "\n",
    "T = np.arange(0, 1, 0.01)\n",
    "plt.plot(T, signal(T, **parameters), '.-k')\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод формул\n",
    "\n",
    "Введём формальную терминологию.\n",
    "\n",
    "Вектор параметров:\n",
    "\n",
    "$$\\theta = [A, f, \\phi]$$\n",
    "\n",
    "Вероятностная модель:\n",
    "\n",
    "$$\\hat{a}_i(\\theta) \\sim \\mathcal{N}\\left(A \\sin(2 \\pi f t_i + \\phi), \\sigma^2 \\right)$$\n",
    "\n",
    "$$\\log p(\\hat{a}_i) = - \\frac{1}{2\\sigma^2} \\left( \\hat{a}_i - A \\sin(2 \\pi f t_i + \\phi)  \\right)^2 - \\log 2 \\pi - \\frac{1}{2} \\log \\sigma$$\n",
    "\n",
    "Матрица Фишера для одного измерения:\n",
    "\n",
    "$$\\mathcal{I}(\\theta) = \\mathrm{<INSERT ~~ SOLUTION>}$$\n",
    "\n",
    "\n",
    "Тогда полная матрица Фишера выражается следующим образом:\n",
    "\n",
    "$$\\mathcal{I} = \\mathrm{<INSERT ~~ SOLUTION>}$$\n",
    "\n",
    "\n",
    "Переопределим: $D_{mi} := \\frac{\\partial a(t_i)}{\\partial \\theta_m}$, тогда:\n",
    "\n",
    "$$\\mathcal{I}_{mn} = \\mathrm{<INSERT ~~ SOLUTION>}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{**parameters, **{'a': 5}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((len(parameters), len(T)))\n",
    "\n",
    "# для каждого параметра\n",
    "for i, argname in enumerate(parameters.keys()):\n",
    "    # для каждого измерения\n",
    "    for k, t in enumerate(T):\n",
    "        # определим функцию по которой будем считать производную\n",
    "        func = lambda x: signal(t, **{**parameters, **{argname: x}})\n",
    "        \n",
    "        # посчитаем производную\n",
    "        D[i, k] = derivative(func, parameters[argname], dx=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на зависимость $D$ для разных переменных. \n",
    "\n",
    "Заметим ещё раз, что по количество строк в матрице $D$ совпадает с длиной вектора $\\theta$, а количество столбцов с количеством измерений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T, signal(T, **parameters), '--k', lw=2, label='signal')\n",
    "\n",
    "for Di, argname in zip(D, parameters.keys()):\n",
    "    plt.plot(T, Di, '.-', label=argname)\n",
    "    \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица $D_{ik}$ показывает как сильно $k$-ое измерение влияет на $i$-ый параметр. К примеру, видно, что на параметр амплитуды сильнее всего влияют значения в пиках синуса. \n",
    "\n",
    "Кроме того, последние точки оказываются более чувствительными к частоте. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посчитаем матрицу Фишера по матрице $D$:\n",
    "$$\\mathcal{I}_{m,n}= \\mathrm{<INSERT ~~ SOLUTION>}$$\n",
    "\n",
    "Посчитаем это с помощью функции из numpy: [```einsum```](http://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = <YOUR CODE>\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ячейчке ниже мы посмотрим на дисперсии оценок на параметры сигнала с помощью Крамера-Рао."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_inv = np.linalg.inv(I)\n",
    "\n",
    "for argname, variance in zip(parameters.keys(), I_inv.diagonal()):\n",
    "    print('{}: {:.2g}'.format(argname, np.sqrt(variance)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А здесь мы воспользуемся функцией `curve_fit`, которая делает автоматический фит по конечной выборке и выдаёт оптимальные параметры и ошибки.\n",
    "\n",
    "Как видно, они достаточно похожи.\n",
    "\n",
    "Стоит отметить, что некорректно сравнивать ошибки посчитанные с помощью Крамера-Рао и `curve_fit`, так как они имеют достаточно разную природу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = signal(T, **parameters) + np.random.randn(T.size) * noise\n",
    "plt.plot(T, S, '.-k')\n",
    "\n",
    "popt, pcov = curve_fit(signal, T, S, p0=list(parameters.values()))\n",
    "\n",
    "for argname, variance in zip(parameters.keys(), pcov.diagonal()):\n",
    "    print('{}: {:.2g}'.format(argname, np.sqrt(variance)))\n",
    "\n",
    "Tl = np.linspace(T[0], T[-1], 10000)\n",
    "plt.plot(Tl, signal(Tl, *popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramer_rao(model, p0, X, noise, show_plot=False):\n",
    "    D = np.zeros((len(p0), X.size))\n",
    "    for i, argname in enumerate(parameters.keys()):\n",
    "        for k, t in enumerate(X):\n",
    "            func = lambda x: signal(t, **{**parameters, **{argname: x}})\n",
    "            D[i,k] = derivative(func, parameters[argname], dx=0.0001)\n",
    "        \n",
    "    if show_plot:\n",
    "        plt.plot(X, model(X, **parameters), '--k', lw=2, label='signal')\n",
    "        for Di, argname in zip(D, parameters.keys()):\n",
    "            plt.plot(T, Di, '.-', label=argname)\n",
    "\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('Зависимость параметра от ошибки измерения в конкретной точки')\n",
    "    \n",
    "    I = <YOUR CODE>\n",
    "    I_inv = np.linalg.inv(I)\n",
    "    \n",
    "    return I_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На графике ниже мы показываем как ошибки в определённых точках влияют на оценку параметров $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "T = np.arange(-1., 1, 0.01)\n",
    "cramer_rao(signal, parameters, T, noise, show_plot=True);\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Вернёмся к задаче с \"относительной приспособленностью\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import polygamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посчитаем матрицу Фишера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_I_matrics(alpha, beta):\n",
    "    I_matrics = <YOUR CODE>\n",
    "    return -np.array(I_matrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_matrics = calc_I_matrics(alpha=alpha_true, beta=beta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Воспользуемся тождеством Рао-Крамера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_matrix = np.linalg.inv(I_matrics) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_matrix[0, 1] / np.sqrt(variance_matrix[0, 0] * variance_matrix[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(variance_matrix[0, 1] / np.sqrt(variance_matrix[0, 0] * variance_matrix[1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_matrix[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_matrix[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
